{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998766054240137"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998766054240137"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import logistic\n",
    "logistic.cdf(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998766054240137"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import expit\n",
    "expit(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def predict(features, weights):\n",
    "    \"\"\"Return 1D array of probaities that the class label == 1\"\"\"\n",
    "    z = np.dot(features, weights)\n",
    "    return sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function(self, theta, x, y):\n",
    "    #compute the cost function for all training sample\n",
    "    m = x.shape[0]\n",
    "    totla_cost = (-1/m) * np.sum(y*np.log(probability(theta, x)) + (1-y) * np.log(1 - probability(theta, x)))\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADIENT DESCENT\n",
    "def update_weights(features, labels, weights, lr):\n",
    "    \"\"\"Vectorized Gradient Descent\"\"\"\n",
    "    N = len(features)\n",
    "    #Get prediction \n",
    "    predictions = predict(features, weights)\n",
    "    #1 transpose feature from (200,3) to (3,200) so we can multiply (200,1) cost matrix return a (3,1) matrix holding a 3 \n",
    "    # partial derivative one for each feature--representing the aggreagate slope of cost fuction across all observations\n",
    "    gradient = np.dot(features.T, predictions-labels)\n",
    "    #3  Take the average cost derivative of each feature\n",
    "    gradient /= N\n",
    "    #4 mltiply the gradient by learning rate\n",
    "    gradient *=lr\n",
    "    #5 Subtract from our weights to minimize the cost\n",
    "    weights -= gradient\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TRAIN\n",
    "def train(features, labels, weights, lr, iters):\n",
    "    cost_history = []\n",
    "    for i in range(iters):\n",
    "        weights = update_weights(features, labels, weights, lr)\n",
    "    #calculate the error for auditing purpose\n",
    "        cost = cost_function(features, labels, weights)\n",
    "        cost_history.append(cost)\n",
    "        #Log progress\n",
    "        if i%1000 == 0:\n",
    "            print(\"iter: \" + str(i) + \"cost: \"+str(cost))\n",
    "    return weights,cost_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CLASSIFY\n",
    "def classify(predictions):\n",
    "    \"\"\"\n",
    "    input- N element array predictions b/w 0 and 1\n",
    "    output -N element array of predictions 0(False) and 1(True)\n",
    "    \"\"\"\n",
    "    decision_boundary = np.vectorize(decision_boundary)\n",
    "    return decision_boundary(predictions).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ASSESS\n",
    "def accuracy(predicted_labels, actual_labels):\n",
    "    diff = predicted_labels - actual_labels\n",
    "    return 1.0 - (float(np.count_non_zero(diff))/len(diff))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fac20ae55bde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Extract Feaures+Labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# Create Train/Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Normalize grades to values between 0 and 1 for more efficient computation\n",
    "normalized_range = sklearn.preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "# Extract Feaures+Labels\n",
    "labels.shape = (100,)  #scikit expect this\n",
    "features = normalized_range.fit_transform(features)\n",
    "# Create Train/Test\n",
    "features_train, features_test, labels_test= train_test_split(features,labels, test_size= 0.4)\n",
    "#Scikit Logistic Regression\n",
    "scikit_log_reg = LogisticRegression()\n",
    "scikit_log_reg.fit(features_train,labels_train)\n",
    "# Score is mean accuracy\n",
    "scikit_score = clf.score(features_test, labels_test)\n",
    "print(\"sckit Score: \", scikit_score)\n",
    "# Our mean accuracy\n",
    "observations, features, labels, weights = run()\n",
    "probabilities = predict(features, weights).flatten\n",
    "classification = classifier(probabilities)\n",
    "our_acc = accuracy(classifications, labels.flatten())\n",
    "print(\"our score: \",our_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
